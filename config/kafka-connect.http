# See https://docs.confluent.io/platform/current/connect/monitoring.html
# to learn more about Connect REST interface

GET http://{{kafka_connect_host}}:{{kafka_connect_port}}/connector-plugins
Accept: application/json

###
GET http://{{kafka_connect_host}}:{{kafka_connect_port}}/connectors
Accept: application/json

###
@connector_id = sink-s3

GET http://{{kafka_connect_host}}:{{kafka_connect_port}}/connectors/{{connector_id}}/status
Accept: application/json

###
GET http://{{kafka_connect_host}}:{{kafka_connect_port}}/connectors/{{connector_id}}/tasks
Accept: application/json

###
@input_topic = "user.assigned"
@s3_region = "eu-west-1"
@s3_bucket_name = "dk-demo-kafka-connect"

PUT http://{{kafka_connect_host}}:{{kafka_connect_port}}/connectors/{{connector_id}}/config
Content-Type: application/json
Accept: application/json

{
  "connector.class": "io.confluent.connect.s3.S3SinkConnector",
  "key.converter": "org.apache.kafka.connect.storage.StringConverter",
  "value.converter": "io.confluent.connect.avro.AvroConverter",
  "value.converter.value.subject.name.strategy": "io.confluent.kafka.serializers.subject.RecordNameStrategy",
  "value.converter.schema.registry.url": "http://schema-registry:8081",
  "tasks.max": "2",
  "topics": {{input_topic}},
  "s3.region": {{s3_region}},
  "s3.bucket.name": {{s3_bucket_name}},
  "flush.size": "5",
  "storage.class": "io.confluent.connect.s3.storage.S3Storage",
  "format.class": "io.confluent.connect.s3.format.json.JsonFormat",
  "schema.generator.class": "io.confluent.connect.storage.hive.schema.DefaultSchemaGenerator",
  "partitioner.class": "io.confluent.connect.storage.partitioner.DefaultPartitioner",
  "transforms": "InsertSchemaMetadata",
  "transforms.InsertSchemaMetadata.type": "com.github.dkoval.hackeda.kafka.connect.transforms.InsertSchemaMetadata$Value",
  "transforms.InsertSchemaMetadata.schema.field": "__schema__"
}
